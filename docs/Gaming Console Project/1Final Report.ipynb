{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"1Final Report.ipynb","provenance":[],"mount_file_id":"1F6z2ndkWdaetwlbM__CPGeH3CZvyP8U2","authorship_tag":"ABX9TyOeQQf5WRbcLXiBoMK1Uze1"},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G7epv5rjkbD8","executionInfo":{"status":"ok","timestamp":1617137293509,"user_tz":360,"elapsed":5596,"user":{"displayName":"Ethan Dark","photoUrl":"","userId":"14959034153389910141"}},"outputId":"5e60be5c-d842-4d5e-bd94-be7dcc8aee04"},"source":["!pip install birdy\n","!pip install ratelimiter"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: birdy in /usr/local/lib/python3.7/dist-packages (0.3.2)\n","Requirement already satisfied: requests-oauthlib>=0.3.2 in /usr/local/lib/python3.7/dist-packages (from birdy) (1.3.0)\n","Requirement already satisfied: requests>=1.2.3 in /usr/local/lib/python3.7/dist-packages (from birdy) (2.23.0)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.3.2->birdy) (3.1.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=1.2.3->birdy) (2020.12.5)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=1.2.3->birdy) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=1.2.3->birdy) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=1.2.3->birdy) (2.10)\n","Requirement already satisfied: ratelimiter in /usr/local/lib/python3.7/dist-packages (1.2.0.post0)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"D1mB-Iu5m-7N"},"source":["import json, os, sys, time\n","from zipfile import ZipFile\n","from birdy.twitter import AppClient, UserClient, TwitterRateLimitError\n","from ratelimiter import RateLimiter\n","\n","CONSUMER_KEY = 'gzufdJHhRm8uep91nBr9yJGwX'\n","CONSUMER_SECRET = 'qMx31ZTe6qmwRrvXeRldE04V8pKeHuBLYnmm991Mk4tOYwdyCq'\n","\n","OUTPUT_DIR = '/content/drive/MyDrive/APRD6343/Network Analysis'\n","MAX_TWEETS = 10000\n","max_id = None\n","_client = None\n","\n","def client(consumer_key=None, consumer_secret=None):\n","  global _client\n","  if consumer_key is None:\n","    consumer_key = CONSUMER_KEY\n","  if consumer_secret is None:\n","    consumer_secret = CONSUMER_SECRET\n","  if _client is None:\n","    _client = AppClient(consumer_key, consumer_secret)\n","    access_token = _client.get_access_token()\n","    _client = AppClient(consumer_key, consumer_secret, access_token)\n","  return _client\n","\n","def limited(until):\n","  duration = int(round(until - time.time()))\n","  print('Rate limited, sleeping for {:d} seconds'.format(duration))\n","\n","@RateLimiter(max_calls=440, period=60*15, callback=limited)\n","def fetch_tweets(query, consumer_key=None, consumer_secret=None):\n","  global max_id\n","  print(f'Fetching: \"{query}\" TO MAX ID: {max_id}')\n","  try:\n","      tweets = client(consumer_key, consumer_secret).api.search.tweets.get(\n","          q=query,\n","          count=100,\n","          max_id=max_id).data['statuses']\n","  except TwitterRateLimitError:\n","    sys.exit(\"You've reach your Twitter API rate limit. Wait 15 minutes before trying again\")\n","  try:\n","    id_ = min([tweet['id'] for tweet in tweets])\n","  except ValueError:\n","    return None\n","  if max_id is None or id_ <= max_id:\n","    max_id = id_ - 1\n","  return tweets\n","\n","def initialize_max_id(file_list):\n","  global max_id\n","  for fn in file_list:\n","    n = int(fn.split('.')[0])\n","    if max_id is None or n < max_id:\n","      max_id = n - 1\n","  if max_id is not None:\n","    print('Found previously fetched tweets. Setting max_id to %d' % max_id)\n","\n","def halt(_id):\n","  print('Reached historically fetched ID: %d' % _id)\n","  print('In order to re-fetch older tweets, remove tweets from ther output directory or output zip file.')\n","  sys.exit('\\n!!IMPORTANT: Tweets older than 7 days will not be re-fetched')\n","\n","def search_twitter(query, consumer_key=None, consumer_secret=None, newtweets=False, dozip=True, verbose=False):\n","  output_dir = os.path.join(OUTPUT_DIR, '_'.join(query.split()))\n","  if not os.path.exists(output_dir):\n","    os.makedirs(output_dir)\n","  if dozip:\n","    fn = os.path.join(output_dir, '%s.zip' % '_'.join(query.split()))\n","    outzip = ZipFile(fn, 'a')\n","  if not newtweets:\n","    if dozip:\n","      file_list = [f for f in outzip.namelist() if f.endswith('.json')]\n","    else:\n","      file_list = [f for f in os.listdir(output_dir) if f.endswith('.json')]\n","    initialize_max_id(file_list)\n","  while True:\n","    try: \n","      tweets = fetch_tweets(\n","          query,\n","          consumer_key=consumer_key,\n","          consumer_secret=consumer_secret)\n","      if tweets is None:\n","        print('Search Completed')\n","        if dozip:\n","          outzip.close()\n","        break\n","      for tweet in tweets: \n","        if verbose:\n","          print(tweet['id'])\n","        fn = '%d.json' % tweet['id']\n","        if dozip:\n","          if fn in (file_list):\n","            outzip.close()\n","            halt(tweet['id'])\n","          else:\n","            outzip.writestr(fn, json.dumps(tweet, indent=4))\n","            file_list.append(fn)\n","        else:\n","          path = os.path.join(output_dir, fn)\n","          if fn in (file_list):\n","            halt(tweet['id'])\n","          else:\n","            with open(path, 'w') as outfile:\n","              json.dump(tweet, outfile, indent=4)\n","            file_list.append(fn)\n","        if len(file_list) >= MAX_TWEETS:\n","          if fn in (file_list):\n","            outzip.close()\n","    except:\n","      if dozip:\n","        outzip.close()\n","      raise\n","\n","  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OYyHal0vwhQp"},"source":["#search_twitter('playstation 5 OR ps5')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TS5sUNrduzBU"},"source":["#search_twitter(\"xbox series x OR xbox series s\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WfuivV0w8Im9"},"source":["#search_twitter(\"nintendoswitch OR nintendo switch\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"909wYWXJDfd3"},"source":["#Mention Networks"]},{"cell_type":"code","metadata":{"id":"MyDTqGaUDfKJ"},"source":["import glob\n","import os\n","import shutil\n","import json\n","import csv \n","import zipfile"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TZ2kDXoNDuwU"},"source":["WORKING_DIR = '/content/drive/MyDrive/APRD6343/Network Analysis'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MhyzuZmQD2r5"},"source":["tweetzipfiles = glob.glob('%s/tweets/*.zip' % WORKING_DIR)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AfEh_S64ECJo","executionInfo":{"status":"ok","timestamp":1617137872053,"user_tz":360,"elapsed":222,"user":{"displayName":"Ethan Dark","photoUrl":"","userId":"14959034153389910141"}},"outputId":"e884f70f-bf79-45d3-cb4f-ec330ae55393"},"source":["tweetzipfiles"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['/content/drive/MyDrive/APRD6343/Network Analysis/tweets/playstation_5_OR_ps5.zip',\n"," '/content/drive/MyDrive/APRD6343/Network Analysis/tweets/nintendoswitch_OR_nintendo_switch.zip',\n"," '/content/drive/MyDrive/APRD6343/Network Analysis/tweets/xbox_series_x_OR_xbox_series_s.zip']"]},"metadata":{"tags":[]},"execution_count":65}]},{"cell_type":"code","metadata":{"id":"YW_KktxLEGem"},"source":["# Do more segmenting\n","\n","uniqueusers = {}\n","\n","for tweetzipfile in tweetzipfiles:\n","  zf = zipfile.ZipFile(tweetzipfile)\n","  for i, obj in enumerate(zf.infolist()):\n","    tweetjson = json.load(zf.open(obj))\n","    userwhotweeted = tweetjson['user']['screen_name']\n","    if userwhotweeted in uniqueusers:\n","      uniqueusers[userwhotweeted] += 1\n","    if userwhotweeted not in uniqueusers:\n","      uniqueusers[userwhotweeted] = 1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WQWpyY5jE7a_","executionInfo":{"status":"ok","timestamp":1617137878321,"user_tz":360,"elapsed":3168,"user":{"displayName":"Ethan Dark","photoUrl":"","userId":"14959034153389910141"}},"outputId":"f4f09d01-ec12-43c6-fd41-b65e4dc9dc10"},"source":["len(uniqueusers)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["21462"]},"metadata":{"tags":[]},"execution_count":67}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"26KdR4l1HK9v","executionInfo":{"status":"ok","timestamp":1617137878323,"user_tz":360,"elapsed":2071,"user":{"displayName":"Ethan Dark","photoUrl":"","userId":"14959034153389910141"}},"outputId":"8ae953f5-b1ed-4404-ad6e-bd6d5978df5a"},"source":["userstoinclude = set()\n","\n","usercount = 0\n","for auser in uniqueusers:\n","  if uniqueusers[auser] > 2:\n","    usercount += 1\n","    userstoinclude.add(auser)\n","\n","print(len(userstoinclude))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["1487\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ClceUCJJAj4L","executionInfo":{"status":"ok","timestamp":1617138051782,"user_tz":360,"elapsed":213,"user":{"displayName":"Ethan Dark","photoUrl":"","userId":"14959034153389910141"}},"outputId":"c7f44337-adfa-40ed-85b1-c76000116c90"},"source":["edgelist = open('%s/console.edgelist.for.gephi.csv' % WORKING_DIR, 'w')\n","csvwriter = csv.writer(edgelist)\n","header = ['Source', 'Target']\n","csvwriter.writerow(header)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["15"]},"metadata":{"tags":[]},"execution_count":80}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XSdRQlX_Bd_b","executionInfo":{"status":"ok","timestamp":1617138058508,"user_tz":360,"elapsed":5221,"user":{"displayName":"Ethan Dark","photoUrl":"","userId":"14959034153389910141"}},"outputId":"d6c51eed-78ad-41d4-d81d-c3b2e0b0bbfc"},"source":["print('Writing edge list')\n","count = 0\n","\n","for tweetzipfile in tweetzipfiles:\n","  count += 1\n","  if count % 1000 == 0:\n","    print(count)\n","  zf = zipfile.ZipFile(tweetzipfile)\n","  for i, obj in enumerate(zf.infolist()):\n","    tweetjson = json.load(zf.open(obj))\n","    userwhotweeted = tweetjson['user']['screen_name']\n","    if userwhotweeted in userstoinclude:\n","      users = tweetjson['entities']['user_mentions']\n","      if len(users) > 0:\n","        for auser in users:\n","          screenname = auser['screen_name']\n","          row = [userwhotweeted, screenname]\n","          csvwriter.writerow(row)\n","edgelist.close()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Writing edge list\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"_z_6G5cWxSdE"},"source":["#Semantic Networks"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ssSeVcEcv1dX","executionInfo":{"status":"ok","timestamp":1617143698057,"user_tz":360,"elapsed":346,"user":{"displayName":"Ethan Dark","photoUrl":"","userId":"14959034153389910141"}},"outputId":"2dd42c64-0fcf-4bd9-f980-6d346223b4c4"},"source":["import nltk\n","wn = nltk.WordNetLemmatizer()\n","ps = nltk.PorterStemmer()\n","import glob\n","import os\n","import re\n","import shutil\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","nltk.download('wordnet')\n","import csv\n","from nltk.stem import WordNetLemmatizer\n","wordnet_lemmatizer = WordNetLemmatizer()\n","import string\n","import itertools\n","import zipfile\n","import json\n","punctuation = string.punctuation\n","stopwordsset = set(stopwords.words(\"english\"))\n","stopwordsset.add('rt')\n","stopwordsset.add(\"'s\")\n","\n","stopwordsset.add(\"amp\")\n","stopwordsset.add(\"ve\")\n","stopwordsset.add('â€¢')\n","emojis = re.compile(pattern = \"[\"\n","        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n","        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n","        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n","        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n","                           \"]+\", flags = re.UNICODE)\n","nltk.download('words')\n","words = set(nltk.corpus.words.words())\n","nltk.download('names')\n","stopwordsset.add(w.lower() for w in nltk.corpus.names.words())"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package words to /root/nltk_data...\n","[nltk_data]   Package words is already up-to-date!\n","[nltk_data] Downloading package names to /root/nltk_data...\n","[nltk_data]   Package names is already up-to-date!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"14KpcuIZv8WM"},"source":["#Removing urls\n","def removeURL(text):\n","  result = re.sub(r\"http\\S+\", \"\", text)\n","  return result\n","\n","#Extracting contextual words from a sentence\n","# tokenizing is taking out all the words in a sentence and turning it into tokens/words\n","def tokenize(text):\n","  #lower case\n","  text = text.lower()\n","  #split into individual words\n","  words = word_tokenize(text)\n","  return words\n","\n","#stem - peaches : peach : reduce the number of repeated words\n","def stem(tokenizedtext):\n","  rootwords = []\n","  for aword in tokenizedtext:\n","    aword = ps.stem(aword)\n","    rootwords.append(aword)\n","  return rootwords\n","\n","#removes useless words such as a, an, the\n","def stopWords(tokenizedtext):\n","  goodwords = []\n","  for aword in tokenizedtext:\n","    if aword not in stopwordsset:\n","      goodwords.append(aword)\n","  return goodwords\n","\n","# feature reduction. taking words and getting their roots and graphing only the root words\n","def lemmatizer(tokenizedtext):\n","  lemmawords = []\n","  for aword in tokenizedtext:\n","    aword = wn.lemmatize(aword)\n","    lemmawords.append(aword)\n","  return lemmawords\n","\n","#inputs a list of tokens and returns a list of unpunctuated tokens/words\n","def removePunctuation(tokenizedtext):\n","  nopunctwords = []\n","  for aword in tokenizedtext:\n","    if aword not in punctuation:\n","      nopunctwords.append(aword)\n","  cleanedwords = []\n","  for aword in nopunctwords:\n","    aword = aword.translate(str.maketrans('', '', string.punctuation))\n","    cleanedwords.append(aword)\n","  return cleanedwords\n","\n","def deEmojify(tokenizedtext):\n","    deemojifiedwords = []\n","    for aword in tokenizedtext:\n","      aword = emojis.sub(r'',aword)\n","      deemojifiedwords.append(aword)\n","    return deemojifiedwords\n","\n","def removeForeign(tokenizedtext):\n","  english = []\n","  for aword in tokenizedtext:\n","    res = [idx for idx in aword if not \n","           re.findall(\"[^\\u0000-\\u05C0\\u2100-\\u214F\\0-9]+\", idx)]\n","    res = ''.join(res)\n","    if res in words or not res.isalpha():\n","      if len(res) > 2:\n","        english.append(res)\n","  return english"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M2P3aBnzHLfy","executionInfo":{"status":"ok","timestamp":1617143724411,"user_tz":360,"elapsed":14058,"user":{"displayName":"Ethan Dark","photoUrl":"","userId":"14959034153389910141"}},"outputId":"86ac4879-fb11-42a6-a468-aebc7dbffd56"},"source":["nltk.download('twitter_samples')\n","from nltk.corpus import twitter_samples\n","\n","positive_tweets = twitter_samples.strings('positive_tweets.json')\n","negative_tweets = twitter_samples.strings('negative_tweets.json')\n","text = twitter_samples.strings('tweets.20150430-223406.json')\n","tweet_tokens = twitter_samples.tokenized('positive_tweets.json')\n","\n","nltk.download('averaged_perceptron_tagger')\n","from nltk.tag import pos_tag\n","from nltk.corpus import twitter_samples\n","\n","def lemmatize_sentence(tokens):\n","    lemmatized_sentence = []\n","    for word, tag in pos_tag(tokens):\n","        if tag.startswith('NN'):\n","            pos = 'n'\n","        elif tag.startswith('VB'):\n","            pos = 'v'\n","        else:\n","            pos = 'a'\n","        lemmatized_sentence.append(wn.lemmatize(word, pos))\n","    return lemmatized_sentence\n","\n","def remove_noise(tweet_tokens, stop_words = ()):\n","    cleaned_tokens = []\n","    for token, tag in pos_tag(tweet_tokens):\n","        token = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|'\\\n","                       '(?:%[0-9a-fA-F][0-9a-fA-F]))+','', token)\n","        token = re.sub(\"(@[A-Za-z0-9_]+)\",\"\", token)\n","        if tag.startswith(\"NN\"):\n","            pos = 'n'\n","        elif tag.startswith('VB'):\n","            pos = 'v'\n","        else:\n","            pos = 'a'\n","        token = wn.lemmatize(token, pos)\n","        if len(token) > 0 and token not in string.punctuation and token.lower() not in stop_words:\n","            cleaned_tokens.append(token.lower())\n","    return cleaned_tokens\n","\n","positive_tweet_tokens = twitter_samples.tokenized('positive_tweets.json')\n","negative_tweet_tokens = twitter_samples.tokenized('negative_tweets.json')\n","positive_cleaned_tokens_list = []\n","negative_cleaned_tokens_list = []\n","\n","for tokens in positive_tweet_tokens:\n","    positive_cleaned_tokens_list.append(remove_noise(tokens, stopwordsset))\n","for tokens in negative_tweet_tokens:\n","    negative_cleaned_tokens_list.append(remove_noise(tokens, stopwordsset))\n","\n","def get_tweets_for_model(cleaned_tokens_list):\n","    for tweet_tokens in cleaned_tokens_list:\n","        yield dict([token, True] for token in tweet_tokens)\n","\n","positive_tokens_for_model = get_tweets_for_model(positive_cleaned_tokens_list)\n","negative_tokens_for_model = get_tweets_for_model(negative_cleaned_tokens_list)\n","\n","import random\n","\n","positive_dataset = [(tweet_dict, \"Positive\")\n","                     for tweet_dict in positive_tokens_for_model]\n","negative_dataset = [(tweet_dict, \"Negative\")\n","                     for tweet_dict in negative_tokens_for_model]\n","dataset = positive_dataset + negative_dataset\n","random.shuffle(dataset)\n","train_data = dataset[:7000]\n","test_data = dataset[7000:]\n","\n","from nltk import classify\n","from nltk import NaiveBayesClassifier\n","classifier = NaiveBayesClassifier.train(train_data)\n","print(\"Accuracy is:\", classify.accuracy(classifier, test_data))\n","print(classifier.show_most_informative_features(10))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n","[nltk_data]   Package twitter_samples is already up-to-date!\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n","Accuracy is: 0.9973333333333333\n","Most Informative Features\n","                      :( = True           Negati : Positi =   2083.4 : 1.0\n","                      :) = True           Positi : Negati =   1641.7 : 1.0\n","                     sad = True           Negati : Positi =     24.2 : 1.0\n","                     bam = True           Positi : Negati =     22.7 : 1.0\n","                follower = True           Positi : Negati =     20.3 : 1.0\n","                     x15 = True           Negati : Positi =     17.2 : 1.0\n","               community = True           Positi : Negati =     16.1 : 1.0\n","                 welcome = True           Positi : Negati =     15.6 : 1.0\n","                  arrive = True           Positi : Negati =     15.6 : 1.0\n","                    damn = True           Negati : Positi =     14.5 : 1.0\n","None\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"woI2i90FwGZA","executionInfo":{"status":"ok","timestamp":1617144591332,"user_tz":360,"elapsed":16373,"user":{"displayName":"Ethan Dark","photoUrl":"","userId":"14959034153389910141"}},"outputId":"cb1e50b4-260c-49ce-ac28-42441ed28ab1"},"source":["from time import sleep \n","\n","uniquewords = {}\n","uniqueposwords = {}\n","uniquenegwords = {}\n","count = 0\n","\n","for tweetzipfile in tweetzipfiles:\n","  zf = zipfile.ZipFile(tweetzipfile)\n","  for i, obj in enumerate(zf.infolist()):\n","    tweetjson = json.load(zf.open(obj))\n","    count += 1\n","    if count % 1000 == 0:\n","        print(count)\n","    \n","    \n","    text = tweetjson['text']\n","    #natural language pre processing : clean the tweet \n","    nourlstext = removeURL(text)\n","    tokenizedtext = tokenize(nourlstext)\n","    nostopwordstext = stopWords(tokenizedtext)\n","    lemmatizedtext = lemmatizer(nostopwordstext)\n","    nopuncttext = removePunctuation(lemmatizedtext)\n","    deemojifiedtext = deEmojify(nopuncttext)\n","    englishtext = removeForeign(deemojifiedtext)\n","    sentiment = classifier.classify(dict([token, True] for token in englishtext))\n","    \n","    #print(tokenizedtext)\n","    #print(nostopwordstext)\n","    #print(lemmatizedtext)\n","    #print(nopuncttext)\n","    #print(deemojifiedtext)\n","    #print(englishtext)\n","    #print(sentiment)\n","    #sleep(2)\n","\n","    for aword in englishtext:\n","        if aword in uniquewords:\n","            uniquewords[aword] += 1\n","        if aword not in uniquewords:\n","            uniquewords[aword] = 1\n","\n","    if sentiment == 'Positive':\n","      for aword in englishtext:\n","        if aword in uniqueposwords:\n","            uniqueposwords[aword] += 1\n","        if aword not in uniqueposwords:\n","            uniqueposwords[aword] = 1\n","\n","    if sentiment == 'Negative':\n","      for aword in englishtext:\n","        if aword in uniquenegwords:\n","            uniquenegwords[aword] += 1\n","        if aword not in uniquenegwords:\n","            uniquenegwords[aword] = 1"],"execution_count":null,"outputs":[{"output_type":"stream","text":["128000\n","129000\n","130000\n","131000\n","132000\n","133000\n","134000\n","135000\n","136000\n","137000\n","138000\n","139000\n","140000\n","141000\n","142000\n","143000\n","144000\n","145000\n","146000\n","147000\n","148000\n","149000\n","150000\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uYB34uovwKGG","executionInfo":{"status":"ok","timestamp":1617144591333,"user_tz":360,"elapsed":12305,"user":{"displayName":"Ethan Dark","photoUrl":"","userId":"14959034153389910141"}},"outputId":"61a240a0-ba45-45c5-93f8-165899bc7013"},"source":["print(len(uniquewords))\n","print(len(uniqueposwords))\n","print(len(uniquenegwords))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["6842\n","4787\n","4115\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"jWNCAG0MwPxI"},"source":["#  list of unique words: key - word; value = number of times the word appears.\n","wordstoinclude = set()\n","wordcount = 0\n","for aword in uniquewords:\n","    if uniquewords[aword] > 18: \n","        wordcount += 1\n","        wordstoinclude.add(aword)\n","\n","poswordstoinclude = set()\n","poswordcount = 0\n","for aword in uniqueposwords:\n","    if uniqueposwords[aword] > 9: \n","        poswordcount += 1\n","        poswordstoinclude.add(aword)\n","\n","negwordstoinclude = set()\n","negwordcount = 0\n","for aword in uniquenegwords:\n","    if uniquenegwords[aword] > 6: \n","        negwordcount += 1\n","        negwordstoinclude.add(aword)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OCXIFKEwwSOQ","executionInfo":{"status":"ok","timestamp":1617144630297,"user_tz":360,"elapsed":235,"user":{"displayName":"Ethan Dark","photoUrl":"","userId":"14959034153389910141"}},"outputId":"2188f7ca-b974-4542-be30-f5e03ab4b31b"},"source":["print(wordcount)\n","print(poswordcount)\n","print(negwordcount)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["930\n","914\n","933\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Fil0UOQywjKc","executionInfo":{"status":"ok","timestamp":1617145374905,"user_tz":360,"elapsed":24749,"user":{"displayName":"Ethan Dark","photoUrl":"","userId":"14959034153389910141"}},"outputId":"fe3c9feb-ea22-4010-d1ae-1de79b755d5b"},"source":["edgelist = open('%s/console.semantic.edgelist.for.gephi.csv' % WORKING_DIR,'w')\n","csvwriter = csv.writer(edgelist)\n","poslist = open('%s/pos.semantic.edgelist.for.gephi.csv' % WORKING_DIR,'w')\n","poswriter = csv.writer(poslist)\n","neglist = open('%s/neg.semantic.edgelist.for.gephi.csv' % WORKING_DIR,'w')\n","negwriter = csv.writer(neglist)\n","\n","header = ['Source', 'Target', 'Type'] # add Type to mention this is undirected. in previous file, it was directed so didnt use it\n","csvwriter.writerow(header)\n","poswriter.writerow(header)\n","negwriter.writerow(header)\n","\n","print('Writing Edge List')\n","\n","uniquewords = {}\n","uniqueposwords = {}\n","uniquenegwords = {}\n","count = 0\n","\n","for tweetzipfile in tweetzipfiles:\n","  zf = zipfile.ZipFile(tweetzipfile)\n","  for i, obj in enumerate(zf.infolist()):\n","    tweetjson = json.load(zf.open(obj))\n","    count += 1\n","    if count % 1000 == 0:\n","        print(count)\n","    \n","    text = tweetjson['text']\n","    #natural language pre processing : clean the tweet \n","    nourlstext = removeURL(text)\n","    tokenizedtext = tokenize(nourlstext)\n","    nostopwordstext = stopWords(tokenizedtext)\n","    lemmatizedtext = lemmatizer(nostopwordstext)\n","    nopuncttext = removePunctuation(lemmatizedtext)\n","    deemojifiedtext = deEmojify(nopuncttext)\n","    englishtext = removeForeign(deemojifiedtext)\n","    sentiment = classifier.classify(dict([token, True] for token in englishtext))\n","\n","    goodwords = []\n","    for aword in englishtext:\n","        if aword in wordstoinclude:\n","            goodwords.append(aword.replace(',', ''))\n","            \n","    allcombos = itertools.combinations(goodwords, 2) # set this to the number of words you want in each combination\n","    for acombo in allcombos:\n","        row = []\n","        for anode in acombo:\n","            row.append(anode)\n","        row.append('Undirected')\n","        csvwriter.writerow(row)\n","\n","    if sentiment == 'Positive':\n","      poswords = []\n","      for aword in englishtext:\n","          if aword in poswordstoinclude:\n","              poswords.append(aword.replace(',', ''))\n","              \n","      allcombos = itertools.combinations(poswords, 2) # set this to the number of words you want in each combination\n","      for acombo in allcombos:\n","          row = []\n","          for anode in acombo:\n","              row.append(anode)\n","          row.append('Undirected')\n","          poswriter.writerow(row)\n","\n","    if sentiment == 'Negative':\n","      negwords = []\n","      for aword in englishtext:\n","          if aword in negwordstoinclude:\n","              negwords.append(aword.replace(',', ''))\n","              \n","      allcombos = itertools.combinations(negwords, 2) # set this to the number of words you want in each combination\n","      for acombo in allcombos:\n","          row = []\n","          for anode in acombo:\n","              row.append(anode)\n","          row.append('Undirected')\n","          negwriter.writerow(row)\n","\n","edgelist.close()\n","poslist.close()\n","neglist.close()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Writing Edge List\n","1000\n","2000\n","3000\n","4000\n","5000\n","6000\n","7000\n","8000\n","9000\n","10000\n","11000\n","12000\n","13000\n","14000\n","15000\n","16000\n","17000\n","18000\n","19000\n","20000\n","21000\n","22000\n","23000\n","24000\n","25000\n","26000\n","27000\n","28000\n","29000\n","30000\n"],"name":"stdout"}]}]}